{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-zVYH3x5D1O"
   },
   "source": [
    "# Lecture 5 : Recurrent Neural Networks\n",
    "### Please access this notebook at: https://github.com/sangaer/PracticalMachineLearning2019\n",
    "\n",
    "In today's course, we'll cover 3 labs:\n",
    "- Lab1. Pre-processing for Text Data\n",
    "- Lab2. Long Short-Term Memory - Spam Message Classification\n",
    "- Lab3. Sequence-to-Sequence Autoencoder - English to French Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2QN753Og5D1P"
   },
   "source": [
    "## Package Import\n",
    "----\n",
    "Import the required package, including numpy, tensorflow, and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "4AIUROSM5D1P",
    "outputId": "2d0aecde-cb85-44b0-dd3b-a0a624fbc9b5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvDKiMAE5D1T"
   },
   "source": [
    "## Lab 1. Pre-processing for Text Data\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyQDE5A35D1T"
   },
   "source": [
    "### Dataset Preparation\n",
    "----\n",
    "The **SMS Spam Collection** Data Set is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,572 messages, tagged acording being ham (legitimate) or spam.\n",
    "\n",
    "For more information, see: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "XXKsxYiT5D1V",
    "outputId": "037ca47a-8bef-4e94-ad54-6662e024a624"
   },
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join('data')\n",
    "dataset_sms_path = os.path.join('data', 'SMSSpamCollection')\n",
    "\n",
    "def download_sms_collection_dataset():\n",
    "    r = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip')\n",
    "    if r.status_code == 200:\n",
    "        f_zip = ZipFile(BytesIO(r.content))\n",
    "\n",
    "        dataset_raw_content = f_zip.read('SMSSpamCollection')\n",
    "        with open(dataset_sms_path, 'wb') as f_dataset:\n",
    "            f_dataset.write(b'label\\tcontent\\n')\n",
    "            f_dataset.write(dataset_raw_content)\n",
    "    else:\n",
    "        assert('Error for downloading SMS Spam Collection Dataset')\n",
    "        \n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "if not os.path.exists(dataset_sms_path):\n",
    "    download_sms_collection_dataset()\n",
    "    \n",
    "df = pd.read_csv(dataset_sms_path, sep='\\t')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncdUzJf85D1Z"
   },
   "source": [
    "### Word Spliting\n",
    "----\n",
    "Convert each sentence into separate words sequence. Notice that not only for the dot (`.`), other stop words (e.g., comma (`,`), colon (`:`), semi-colon (`;`)., etc.) are also possible separators.\n",
    "> **Your task**:  \n",
    "> Spliting each sentence into words list by using `tf.keras.preprocessing.text.text_to_word_sentence`. For example, for the following sentence:\n",
    "> - Recurrent neural networks have a wide array of applications, including time series analysis, document classification, speech and voice recognition.\n",
    "> \n",
    "> The expected result is:\n",
    "> - `['recurrent', 'neural', 'networks', 'have', 'a', 'wide', 'array', 'of', 'applications', 'including', 'time', 'series', 'analysis', 'document', 'classification', 'speech', 'and', 'voice', 'recognition']`\n",
    "> ---\n",
    "> In the mean while, adjust the argument `MAX_SEQUENCE_LENGTH` based on the observation of the sequence length histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "qfBiPIwm5D1a",
    "outputId": "13b78faa-e1a9-4de8-83c2-61439f45ab16"
   },
   "outputs": [],
   "source": [
    "# Finish your part here\n",
    "# 1. Your input is df['content'], use `for x_str in df['content']` for iterative processing\n",
    "#     e.g.,\n",
    "#     df['content'] = [\n",
    "#         'abc def',\n",
    "#         'gh, ijkl; mnop, qrs.',\n",
    "#         'tu! vwx, yz!'\n",
    "#     ]\n",
    "#\n",
    "# 2. Expected result should be a Python list `X_str`, with each element a variable-length array\n",
    "#     e.g.,\n",
    "#     X_str == [\n",
    "#         ['abc', 'def'], \n",
    "#         ['gh', 'ijkl', 'mnop', 'qrs'],\n",
    "#         ['tu', 'vwx', 'yz']\n",
    "#     ]\n",
    "\n",
    "\n",
    "# The following part validate if your result is correct\n",
    "for i in range(0, 20):\n",
    "    print('[\\'{}\\'{}]'.format('\\', \\''.join(X_str[i][0:10]), ' ...' if len(X_str[i]) > 10 else ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "heKg8V875D1c",
    "outputId": "0bfe25d0-6991-43ef-9035-4dbf45bed3da"
   },
   "outputs": [],
   "source": [
    "plt.hist([len(x_str) for x_str in X_str], bins=100)\n",
    "plt.ylabel('occurrences')\n",
    "plt.xlabel('Number of Words in Sentence')\n",
    "plt.show()\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cU7SKurF5D1e"
   },
   "source": [
    "### Tokenization\n",
    "----\n",
    "Use `keras.preprocessing.text.Tokenizer` to encode each word into integer-based code according to word occurrences.\n",
    "\n",
    "> **Your task**:  \n",
    "> Adjust the argument `MAX_NUM_DICT_WORDS`, make sure the uncommon words (e.g., occurrences < 10) can be removed during the string-based to integer-based code conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-EdkvOQ5D1f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adjust this argument by observing the word occurrence\n",
    "MAX_NUM_DICT_WORDS = 2000\n",
    "\n",
    "# Finish your part here. The variable tokenizer is expected to be an instance of tf.keras.preprocessing.text.Tokenizer\n",
    "# ....\n",
    "\n",
    "# The following part validate if your result is correct\n",
    "tokenizer.fit_on_texts(X_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "2qN1T2up5D1g",
    "outputId": "d14a69a1-9ad9-45dd-8067-a279e2eb6b47"
   },
   "outputs": [],
   "source": [
    "def display_tokenizer_stat(tokenizer, max_items=None):\n",
    "    list_tokenizer_stat_keys, list_tokenizer_stat_words, list_tokenizer_stat_cnts = [], [], []\n",
    "    for key in sorted(tokenizer.index_word):\n",
    "        word = tokenizer.index_word[key]\n",
    "        cnt = -1\n",
    "        if word != 'UNK':\n",
    "            cnt = tokenizer.word_counts[word]\n",
    "        list_tokenizer_stat_keys.append(key)\n",
    "        list_tokenizer_stat_words.append(word)\n",
    "        list_tokenizer_stat_cnts.append(cnt)\n",
    "\n",
    "    df_tokenizer_stat = pd.DataFrame(columns=['Key', 'Word', 'Occurrences'],\n",
    "                                     data={\n",
    "                                         'Key': list_tokenizer_stat_keys,\n",
    "                                         'Word': list_tokenizer_stat_words,\n",
    "                                         'Occurrences': list_tokenizer_stat_cnts\n",
    "                                     })\n",
    "    if max_items is not None:\n",
    "        display(df_tokenizer_stat[:max_items])\n",
    "    else:\n",
    "        display(df_tokenizer_stat)\n",
    "\n",
    "\n",
    "# display here\n",
    "display_tokenizer_stat(tokenizer, MAX_NUM_DICT_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FR3W2US5D1i"
   },
   "source": [
    "Now, use the pre-trained tokenizer to convert **string-based** sequences into **integer-based** sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "WNS0e7Z55D1i",
    "outputId": "81395358-ab0d-4fe0-9605-5b090d62ef67"
   },
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(X_str)\n",
    "for x in X[:20]:\n",
    "    print('[{}]'.format(', '.join([str(x_) for x_ in x])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "muSyNabc5D1k"
   },
   "source": [
    "### Sequences Padding\n",
    "----\n",
    "Most of deep learning framework working relies on batch processing. A fix-sized sequences dataset are definitely helpful for this. Therefore, we use sequence padding help function `tf.keras.preprocessing.sequence.pad_sequences` to convert variable-length sequences into fixed-length sequences.\n",
    "\n",
    "> **Your task**:  \n",
    "> Use `tf.keras.preprocessing.sequence.pad_sequences`, padding or truncating the sequences `X` into fixed-length array. Your length should be `MAX_SEQUENCE_LENGTH`.\n",
    "> \n",
    "> Reference: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "TdtUMDnb5D1l",
    "outputId": "d97176f1-dddc-4270-9f9d-eb0227a7235b"
   },
   "outputs": [],
   "source": [
    "# Finish your part here\n",
    "# ...\n",
    "\n",
    "# The following part validate if your result is correct\n",
    "for i in range(0, 10):\n",
    "    print('{}'.format(X[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmPxiMJc5D1m"
   },
   "source": [
    "### Label Encoder\n",
    "----\n",
    "Use `sklearn.preprocessing.LabelEncoder` to convert **string-based** label into **integer-based** label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NRmB6Iu85D1n",
    "outputId": "d8646246-92a9-402b-e07e-30067ae74d65"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df['label'])\n",
    "\n",
    "Y = label_encoder.transform(df['label'])\n",
    "\n",
    "display( [ (idx, label) for idx, label in enumerate(label_encoder.classes_) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAkSZgiI5D1o"
   },
   "source": [
    "Now you already have cleaned, and good shaped data. Split it into training and testing set for further evaluation.\n",
    "\n",
    "- **Training Set**  \n",
    "    A set of examples used for learning the model parameters (i.e., weights or bases) of the classifier.\n",
    "\n",
    "    The pair of data and corresponded label are expressed as variable (`X_train`, `Y_train`)\n",
    "- **Testing Set**  \n",
    "    A set of examples used for performance evaluation only.\n",
    "\n",
    "    The pair of data and corresponded label are expressed as variable (`X_test`, `Y_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_sQ_Ox75D1p"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4edYBK155D1r"
   },
   "source": [
    "## Lab 2. Long Short-Term Memory (LSTM) - Spam Message Classification\n",
    "----\n",
    "Remember the following arguments:\n",
    "- `MAX_NUM_DICT_WORDS`:  \n",
    "    All possible words which may be appeared in the sentence dataset, which convert the word to the one-hot encoded space. The argument is used by tokenizer and embedding layer.\n",
    "    \n",
    "    If the word occurrence is less than the last item kept in the tokenizer, the word will be removed and replaced by `UNK`.\n",
    "- `MAX_SEQUENCE_LENGTH`:  \n",
    "    The maximum length of the sequences.\n",
    "- `WORD_EMBED_DIMENSION`:  \n",
    "    The dimension of the projected space of embedding layer.\n",
    "    \n",
    "Now, we use **LSTM** and **Embedding** layer to construct the sequence classifier.\n",
    "\n",
    "For more details, please refer to **week5.pptx**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "colab_type": "code",
    "id": "8jsp51ad5D1s",
    "outputId": "6a4a4f87-9f89-4aaa-e3a2-81917612f730"
   },
   "outputs": [],
   "source": [
    "WORD_EMBED_DIMENSION = 256\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(MAX_NUM_DICT_WORDS, WORD_EMBED_DIMENSION, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRaapkq25D1u",
    "outputId": "42b01140-4139-4645-ee73-1af3d7972632"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=10, validation_data=(X_test, Y_test), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TSiX04_h5D1w",
    "outputId": "cfc62183-f025-4091-c262-498aa6f568d9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['acc'], label='Training accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uH0QOP7Y5D1y"
   },
   "source": [
    "## Lab 3. Sequence-to-Sequence Autoencoder - English to French Translation\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6S0je4C5D1y"
   },
   "source": [
    "### Dataset Preparation\n",
    "----\n",
    "The **Tatoeba** is a collection of sentences and translations. The subset `fra-eng` contains 160872 english to french sentence pair. Here, we use the first 10000 sentences for sequence to sequence autoencoder training and prediction.\n",
    "\n",
    "For more information, please refer to:\n",
    "- https://tatoeba.org/eng\n",
    "- http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "oW3NOKJ85D1z",
    "outputId": "5de21a85-4367-48ee-b485-8445dbf80cec"
   },
   "outputs": [],
   "source": [
    "dataset_fra_eng_path = os.path.join(dataset_dir, 'fra.txt')\n",
    "\n",
    "def download_fra_eng_dataset():\n",
    "    if not os.path.exists('data/fra.txt'):\n",
    "        if not os.path.exists('~/kaggle.json'):\n",
    "            !mkdir ~/.kaggle\n",
    "            !cp kaggle.json ~/.kaggle/\n",
    "            !chmod 600 ~/.kaggle/kaggle.json\n",
    "        !kaggle datasets download -d myksust/fra-eng\n",
    "        !unzip fra-eng.zip\n",
    "        !rm _about.txt\n",
    "        !rm fra-eng.zip\n",
    "        !mv fra.txt ./data\n",
    "\n",
    "def prepare_fra_eng_dataset():\n",
    "    with open(dataset_fra_eng_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    \n",
    "    encoder_input_data_str, decoder_input_data_str, decoder_output_data_str = [], [], []\n",
    "    \n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for idx, line in enumerate(lines):\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if idx == 10000:\n",
    "            break\n",
    "        source, target = line.split('\\t')\n",
    "\n",
    "        encoder_input_data_str.append(source)\n",
    "        decoder_input_data_str.append('\\t' + target)\n",
    "        decoder_output_data_str.append(target + '\\n')\n",
    "        \n",
    "    return encoder_input_data_str, decoder_input_data_str, decoder_output_data_str\n",
    "        \n",
    "download_fra_eng_dataset()\n",
    "encoder_input_data_str, decoder_input_data_str, decoder_output_data_str = prepare_fra_eng_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EAVf7bTR5D11",
    "outputId": "ad42a781-c131-47f1-eb54-4d445a25893e"
   },
   "outputs": [],
   "source": [
    "df_eng_fra = pd.DataFrame({'English': encoder_input_data_str[:50],\n",
    "                           'French (input)': decoder_input_data_str[:50],\n",
    "                           'French (outputs)': decoder_output_data_str[:50]})\n",
    "display(df_eng_fra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Je6XBNUJ5D12"
   },
   "source": [
    "> **Your task**:  \n",
    "> In the function `prepare_fra_eng_dataset`, both English sentences (source) and French sentences (target) have been splited into word sequences. Please adjust the argument `MAX_SEQUENCE_LENGTH` based on the observation of the sequence length histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "IwS1EhWc5D13",
    "outputId": "f6b123b2-7598-4173-9d46-902ca2fe09c7"
   },
   "outputs": [],
   "source": [
    "plt.hist([len(x_str) for x_str in encoder_input_data_str], bins=10)\n",
    "plt.ylabel('occurrences')\n",
    "plt.xlabel('Number of Words in Sentence (English)')\n",
    "plt.show()\n",
    "\n",
    "plt.hist([len(x_str) for x_str in decoder_output_data_str], bins=10)\n",
    "plt.ylabel('occurrences')\n",
    "plt.xlabel('Number of Words in Sentence (French)')\n",
    "plt.show()\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_ENG = 16\n",
    "MAX_SEQUENCE_LENGTH_FRA = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTRv5QEp5D16"
   },
   "source": [
    "### Tokenization\n",
    "----\n",
    "Use `keras.preprocessing.text.Tokenizer` to encode each word into integer-based code according to word occurrences.\n",
    "\n",
    "**Notice:** Due to scale of the dataset is relatively small, we use **Character-based** tokenization\n",
    "\n",
    "> **Your task**:  \n",
    "> Adjust the argument `MAX_NUM_DICT_TOKENS_ENG` and `MAX_NUM_DICT_TOKENS_FRA` for both english and french, make sure the uncommon words (e.g., occurrences < 10) can be removed during the string-based to integer-based code conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_Sdf6IhQ5D19",
    "outputId": "6f9ca789-270c-4fef-825d-fc1f4854ee00",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_DICT_TOKENS_ENG = 45\n",
    "MAX_NUM_DICT_TOKENS_FRA = 66\n",
    "\n",
    "\n",
    "# English part\n",
    "tokenizer_eng = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_DICT_TOKENS_ENG, char_level=True)\n",
    "tokenizer_eng.fit_on_texts(encoder_input_data_str)\n",
    "\n",
    "encoder_input_data = tokenizer_eng.texts_to_sequences(encoder_input_data_str)\n",
    "display_tokenizer_stat(tokenizer_eng, MAX_NUM_DICT_TOKENS_ENG)\n",
    "\n",
    "\n",
    "# French part\n",
    "tokenizer_fra = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_DICT_TOKENS_FRA, char_level=True)\n",
    "tokenizer_fra.fit_on_texts([sentence + '\\n' for sentence in decoder_input_data_str])\n",
    "\n",
    "decoder_input_data = tokenizer_fra.texts_to_sequences(decoder_input_data_str)\n",
    "decoder_output_data = tokenizer_fra.texts_to_sequences(decoder_output_data_str)\n",
    "display_tokenizer_stat(tokenizer_fra, MAX_NUM_DICT_TOKENS_FRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3psaT5q5D2A"
   },
   "source": [
    "### Sequences Padding\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6PDyoLk5D2A"
   },
   "outputs": [],
   "source": [
    "encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(encoder_input_data,\n",
    "                                                                   maxlen=MAX_SEQUENCE_LENGTH_ENG,\n",
    "                                                                   padding='post',\n",
    "                                                                   truncating='post')\n",
    "encoder_input_data = tf.keras.utils.to_categorical(encoder_input_data)\n",
    "\n",
    "decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_input_data,\n",
    "                                                                   maxlen=MAX_SEQUENCE_LENGTH_FRA,\n",
    "                                                                   padding='post',\n",
    "                                                                   truncating='post')\n",
    "decoder_input_data = tf.keras.utils.to_categorical(decoder_input_data)\n",
    "\n",
    "\n",
    "decoder_output_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_output_data,\n",
    "                                                                    maxlen=MAX_SEQUENCE_LENGTH_FRA,\n",
    "                                                                    padding='post',\n",
    "                                                                    truncating='post')\n",
    "decoder_output_data = tf.keras.utils.to_categorical(decoder_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QVQDsjIF5D2C"
   },
   "source": [
    "### Sequence to Sequence Autoencoder Model\n",
    "----\n",
    "Please refer to **week5.pptx** for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtpM5uTQ5D2C"
   },
   "source": [
    "#### Create Model and Training\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "colab_type": "code",
    "id": "6STQ2S3d5D2D",
    "outputId": "e6534331-5209-487b-a028-26c61e4551ca",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LSTM_LATENT_DIMENSION = 256\n",
    "\n",
    "\n",
    "# ENCODER\n",
    "encoder_input = tf.keras.layers.Input(shape=(None, MAX_NUM_DICT_TOKENS_ENG),\n",
    "                                      name='encoder_input')\n",
    "encoder_lstm = tf.keras.layers.CuDNNLSTM(LSTM_LATENT_DIMENSION,\n",
    "                                    return_state=True,\n",
    "                                    name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm (encoder_input)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# DECODER\n",
    "decoder_input = tf.keras.layers.Input(shape=(None, MAX_NUM_DICT_TOKENS_FRA),\n",
    "                                      name='decoder_input')\n",
    "decoder_lstm = tf.keras.layers.CuDNNLSTM(LSTM_LATENT_DIMENSION,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    name='decoder_lstm')\n",
    "decoder_lstm_out, _, _ = decoder_lstm(decoder_input,\n",
    "                                      initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(MAX_NUM_DICT_TOKENS_FRA,\n",
    "                                      activation='softmax',\n",
    "                                      name='decoder_dense')\n",
    "decoder_outputs = decoder_dense (decoder_lstm_out)\n",
    "\n",
    "\n",
    "# MODEL\n",
    "model = tf.keras.models.Model([encoder_input, decoder_input], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZfZMSBPs5D2E",
    "outputId": "b00977b9-2a06-4ab9-d806-e155a7e25665",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit([encoder_input_data, decoder_input_data],\n",
    "                    decoder_output_data,\n",
    "                    batch_size=64,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "id": "giM9gcah5D2G",
    "outputId": "ca09fc92-50e4-4d88-b08a-1366af5f0542"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ex4uMEu5D2H"
   },
   "source": [
    "#### Predict Sentence by Pre-trained Sequence to Sequence Model\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "colab_type": "code",
    "id": "ujPCC1pu5D2H",
    "outputId": "3d9e6632-74ee-4a6c-be13-87a2cf2983a5"
   },
   "outputs": [],
   "source": [
    "encoder_model = tf.keras.models.Model(encoder_input, encoder_states)\n",
    "\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(LSTM_LATENT_DIMENSION,),\n",
    "                                              name='decoder_state_input_h')\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(LSTM_LATENT_DIMENSION,),\n",
    "                                              name='decoder_state_input_c')\n",
    "decoder_states_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm_out, state_h, state_c = decoder_lstm(decoder_input, initial_state=decoder_states_input)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_output = decoder_dense(decoder_lstm_out)\n",
    "\n",
    "decoder_model = tf.keras.models.Model([decoder_input] + decoder_states_input,\n",
    "                                      [decoder_output] + decoder_states)\n",
    "decoder_model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(decoder_model, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gs2B9FDK5D2J"
   },
   "outputs": [],
   "source": [
    "token_fra_start_idx = tokenizer_fra.texts_to_sequences([['\\t']])[0][0]\n",
    "token_fra_end_idx = tokenizer_fra.texts_to_sequences([['\\n']])[0][0]\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Get the states h and c from source language (English)\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    stop_condition = False\n",
    "    \n",
    "    # Decoded sentence - One-hot array\n",
    "    decoded_sentence_onehot = np.zeros((1, 1, MAX_NUM_DICT_TOKENS_FRA))\n",
    "    decoded_sentence_onehot[0, 0, token_fra_start_idx] = 1\n",
    "    \n",
    "    # Decoded sentence - Integer code-based sequence\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([decoded_sentence_onehot] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "        decoded_sentence += tokenizer_fra.index_word[token_idx]\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (token_idx == token_fra_end_idx or len(decoded_sentence) >= MAX_SEQUENCE_LENGTH_FRA):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        decoded_sentence_onehot = np.zeros((1, 1, MAX_NUM_DICT_TOKENS_FRA))\n",
    "        decoded_sentence_onehot[0, 0, token_idx] = 1\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    # Decoded sentence - Integer code-based sequence to Character-based sequence\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rVdN5Oae5D2K",
    "outputId": "94321f89-3f22-4d05-a309-5d74cd128816"
   },
   "outputs": [],
   "source": [
    "decoder_output_data_str_predicted = []\n",
    "for i in range(0, 50):\n",
    "    decoder_output_data_str_predicted.append(decode_sequence(encoder_input_data[i:i+1, :]))\n",
    "\n",
    "df_eng_fra_predict = pd.DataFrame({'English': encoder_input_data_str[:50],\n",
    "                                   'French (Actual)': decoder_input_data_str[:50],\n",
    "                                   'French (Predict)': decoder_output_data_str_predicted[:50]})\n",
    "display(df_eng_fra_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Once finished, please Submit Your Colab Notebook [Here](https://forms.gle/2VZkhTyH59hZhD249)**\n",
    "\n",
    "----\n",
    "\n",
    "**NOTICE:** It takes time to train a sequence to sequence autoencoder. For your convenience:\n",
    "- Both Lab1 and Lab2 are required\n",
    "- Lab3 is optional"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[Lecture5]RecurrentNeuralNetwork.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
