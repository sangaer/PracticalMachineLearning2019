{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 : Convolutional Neural Networks\n",
    "### Please access this notebook at: https://github.com/sangaer/PracticalMachineLearning2019\n",
    "\n",
    "In today's course, we'll cover 3 labs:\n",
    "- Lab1. Recap for Neural Network\n",
    "- Lab2. Convolutional Neural Network\n",
    "- Lab3. Transfer learning by Using a Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Import\n",
    "----\n",
    "Import the required package, including numpy, tensorflow, and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "----\n",
    "Download dataset from kaggle `tongpython/cat-and-dog`, then divided into training, validation, and testing set:\n",
    "- **Training Set**  \n",
    "    A set of examples used for learning the model parameters (i.e., **weights** or **bases**) of the classifier.\n",
    "\n",
    "    The pair of data and corresponded label are expressed as variable (`X_train`, `Y_train`)\n",
    "- **Validation Set**  \n",
    "    A set of examples used for the parameter tuning. Notice that the term \"parameter\" here denotes the architecture of the model (e.g., Number of hidden layers, number of units in a layer ., etc.), instead of the model parameters.\n",
    "\n",
    "    The pair of data and corresponded label are expressed as variable (`X_val`, `Y_val`)\n",
    "- **Testing Set**  \n",
    "    A set of examples used for performance evaluation only.\n",
    "\n",
    "    The pair of data and corresponded label are expressed as variable (`X_test`, `Y_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d tongpython/cat-and-dog\n",
    "!unzip -q cat-and-dog.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_parse(folders, size=(32, 32)):\n",
    "    if isinstance(folders, str):\n",
    "        folders = [folders, ]\n",
    "\n",
    "    X, Y = [], []\n",
    "    for folder in folders:\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            for f in filter(lambda x: x.endswith('.jpg'), files):\n",
    "                img = tf.keras.preprocessing.image.load_img(os.path.join(root, f), target_size=size)\n",
    "                img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "                \n",
    "                label = os.path.basename(root)\n",
    "                \n",
    "                X.append(img)\n",
    "                Y.append(1 if label == 'cats' else 0)\n",
    "    return np.array(X), np.array(Y, dtype='uint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test ='test_set/test_set/'\n",
    "data_train ='training_set/training_set/'\n",
    "\n",
    "X32, Y32 = dataset_parse([data_test, data_train], size=(32, 32))\n",
    "\n",
    "i = 9000\n",
    "j = 9200\n",
    "X_train, X_val, X_test = X32[:i, :], X32[i:j, :], X32[j:, :]\n",
    "Y_train, Y_val, Y_test = Y32[:i], Y32[i:j], Y32[j:]\n",
    "\n",
    "print('Shape of data:')\n",
    "display([X_train.shape, X_val.shape, X_test.shape])\n",
    "\n",
    "print('Shape of label:')\n",
    "display([Y_train.shape, Y_val.shape, Y_test.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(0, 20):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(X_train[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten (32, 32, 3) to the 3072-dimension vector for subsequent usage\n",
    "X_train = np.array([x.flatten() for x in X_train])\n",
    "X_val = np.array([x.flatten() for x in X_val])\n",
    "X_test = np.array([x.flatten() for x in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1. Recap for Neural Network\n",
    "----\n",
    "In this section, we will recap the properties of layers introduced in week1-3. Afterward, you will be asked to finish the following task for the `cat-and-dog` dataset training:\n",
    "1. Compose a Keras/Tensorflow **sequential** model\n",
    "2. Compose a Keras/Tensorflow **functional** model\n",
    "\n",
    "In this example, we construct both types of model by adding 4 fully-connected layer, which is a **Multi-Layer Perceptron** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(2048, input_shape=(X_train.shape[1], ), activation='relu'),\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional Model\n",
    "----\n",
    "> **Your task:** Complete the following functional model\n",
    "> \n",
    "> Reference: https://www.tensorflow.org/guide/keras/functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(3072,))\n",
    "# ... Finish your part here ...\n",
    "outputs = tf.keras.layers.Dense(2, activation='softmax') (layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, here's something you already know from your hand-crafted neural network on week 2:\n",
    "### Recap 1. Basics of Neural Network\n",
    "---\n",
    "In the neural network, **layers** served as a role of learning the knowledge, the common statistic patterns, or features from your training data. Each layer is responsible for predict the properties from the input, and then generate an output with the interesting properties with respect to the target problem.\n",
    "\n",
    "Different layer types provide the different capabilities of agglomerating statistics characteristics from input data. For example, the **Dense layer** (also known as a fully connected layer in most textbooks) of Keras and Tensorflow takes the whole input data into considerations, evaluates its similarity to the model weights across all dimensions, and generates the output to the next layer.\n",
    "\n",
    "To train the model parameters of these layers in order to fit into the target problem from the training data, the first-order derivative of the loss function with respect to the model parameters are required to be evaluated during the training. Take the Dense layer, for example, the loss function of each neuron in the Dense layer with respect to the model parameter $\\mathbf{w}$ can be written in the following form:\n",
    "$$\n",
    "J\\left(\\mathbf{w}\\right) = \\sum_{\\mathbf{x}\\in Y}\\delta_{\\mathbf{x}} \\mathbf{w}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{x} \\in Y$ denotes the incorrectly handled sample. To minimize the loss function $J\\left(\\mathbf{w}\\right)$, a good $\\mathbf{w}$ can be found via gradient descent optimization:\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} \\gets \\mathbf{w} - \\mu\\cdot\\frac{\\partial J\\left(\\mathbf{w}\\right)}{\\partial\\mathbf{w}}\n",
    "$$\n",
    "\n",
    "which requires evaluating the first-order derivative during the back-propagation (i.e., training) procedures:\n",
    "$$\n",
    "\\frac{\\partial J\\left(\\mathbf{w}\\right)}{\\partial\\mathbf{w}} = \\sum_{\\mathbf{x}\\in Y}\\delta_{\\mathbf{x}} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "\n",
    "In addition to the **Dense** layer, we'll also covers **Convolutional Layer**, **Pooling Layer**, **Dropout Layer**, and **Batch Normalization Layer** today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap 2. Activation Layers\n",
    "---\n",
    "An activation function in the neural network architecture served as a role to determine if the particular neuron should be activated or not. In the multi-layer neural network architecture, it is also an important part to provide the ability to describe the non-linear distributed data. Basically, the activation function usually thresholds the input value $x$ to the range of $[0, x]$, $[0,1]$ or $[-1, 1]$ in order to guarantee the training stability. Ideally, a step function can be served as an activation function:\n",
    "$$\n",
    "\\sigma\\left(x\\right) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1 \\text{, if } \\mathbf{x} \\geq 0 \\\\\n",
    "        0 \\text{, otherwise}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "y = [0 if x_ <= 0 else 1 for x_ in x]\n",
    "\n",
    "plt.plot(x, y, label='step(x)')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since there's **no first-order derivative** for the step function, the gradients cannot be evaluated, which makes the step function cannot be used in the neural network architecture. The differentiable function which provides similar capability is something required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid (Logistic) Layer\n",
    "The sigmoid function, also known as logistic function, uses an Euler number to reaches a similar result of the step function. Severed as an activation layer in neural network architecture, it can be written in the following form:\n",
    "- Feed-forward:  \n",
    "    $$\\sigma_s(x) = \\frac{1}{1+e^{-x}}$$\n",
    "- Gradients:  \n",
    "    $$\\frac{\\partial\\sigma_s(x)}{\\partial x} = \\sigma(x)(1-\\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "\n",
    "y = tf.keras.backend.eval(tf.keras.activations.sigmoid(x))\n",
    "y = np.array(y).flatten()\n",
    "\n",
    "y_= y*(1-y)\n",
    "\n",
    "plt.plot(x, y, label='sigmoid(x)')\n",
    "plt.plot(x, y_, label='sigmoid(x)\\'')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU Layer\n",
    "With a simplified form, the Rectified Linear Unit, can be written in the following form:  \n",
    "- Feed-forward:\n",
    "    $$\n",
    "    \\sigma_R\\left(x\\right) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            x \\text{, if } \\mathbf{x} \\geq 0 \\\\\n",
    "            0 \\text{, otherwise}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "    $$\n",
    "- Gradients:\n",
    "    $$\n",
    "    \\frac{\\partial\\sigma_R(x)}{\\partial x} = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 \\text{, if } \\mathbf{x} \\geq 0 \\\\\n",
    "            0 \\text{, otherwise}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "\n",
    "y = tf.keras.backend.eval(tf.keras.activations.relu(x))\n",
    "y = np.array(y).flatten()\n",
    "\n",
    "y_= np.zeros(x.shape)\n",
    "y_[x > 0] = 1\n",
    "y_[x <= 0] = 0\n",
    "\n",
    "plt.plot(x, y, label='relu(x)')\n",
    "plt.plot(x, y_, label='relu(x)\\'')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Sigmoid vs. ReLU?**  \n",
    "> The ReLU function is one of the most widely used activation function in many neural network-based models. Not only because of its simplifications which reduces the computational cost, but there's also some advantages compared with the Sigmoid:\n",
    "> - ReLU creates sparsity, leading the non-activated elements as 0, which is a nice property because only limited neurons are expected to be stimulated in the neural network. The Sigmoid, however, leads a lot of non-zero elements even though the input is relatively small or relatively large\n",
    "> - Sigmoid causes gradient vanishing problem. When the input is far from the threshold, the gradient then soon drops close to 0, which makes the neural network hard to be trained. The ReLU, however, contains a constant gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Layer\n",
    "A softmax activation function evaluates the activation by normalizing all elements from the input vectors, then generates probabilistic results which summed up to 1. Considering with the input $\\mathbf{x}$, the probability that $\\mathbf{x}$ belongs to the class $k$ can be written in the following form:\n",
    "$$\n",
    "P\\left(y=k | \\mathbf{x} \\right) = \\frac{\\exp\\left(\\mathbf{w}_k^T\\mathbf{x}\\right)}{\\sum_{j=1}^{K}\\exp\\left(\\mathbf{w}_j^T\\mathbf{x}\\right)}\n",
    "$$\n",
    "Notice that comparing to the sigmoid and ReLU function, which only takes value into consideration, the softmax considers all elements from the input vectors for normalization purposes. Therefore, the softmax function is usually used for the **multi-class classification** task. In addition, due to its native behavior of normalization, and the vector squashing (i.e., thresholding the input value in the range of $[0,1]$), the softmax is NOT suggested to be used as an output of multi-label multiclass classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap 3. Loss Function\n",
    "---\n",
    "In the neural network architecture, the loss function evaluates how good the output generated by the machine learning model is. More generally, the loss function is the differences between **expected (actual) output** and the **predicted output**.\n",
    "\n",
    "Notice that for the following code snippet:\n",
    "```python\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "**DO NOT** get confused with loss function and accuracy function, even though it looks like both of them can be used for evaluating the model performance. Here are some major differences:\n",
    "- **The loss function is for computers**  \n",
    "    The choice of loss function affects the performance of the training procedure since the loss and gradients are required to be back-propagated into the layers in the fronts in order to fine-tune the model parameters. Differences in the loss value are only relative to the current task. One CANNOT judge the performance of the model by the range of the loss function\n",
    "- **The accuracy function is for human**  \n",
    "    The choice of accuracy function does NOT affect the performance of the training procedure. The model performance can be evaluated by human via this metrics.\n",
    "\n",
    "---\n",
    "Now, we can start training, observing the training loss and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_val, Y_val), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['acc'], label='Training accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap 4. Over-fitting vs. Under-fitting\n",
    "----\n",
    "Read the slide **week4.pptx** for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2. Convolutional Neural Network\n",
    "----\n",
    "With the convolutional layer, we can have a much larger size of the input. Let's prepare for the dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test ='test_set/test_set/'\n",
    "data_train ='training_set/training_set/'\n",
    "\n",
    "X160, Y160 = dataset_parse([data_test, data_train], size=(160, 160))\n",
    "\n",
    "i = 9000\n",
    "j = 9200\n",
    "X_train, X_val, X_test = X160[:i, :], X160[i:j, :], X160[j:, :]\n",
    "Y_train, Y_val, Y_test = Y160[:i], Y160[i:j], Y160[j:]\n",
    "\n",
    "print('Shape of data:')\n",
    "display([X_train.shape, X_val.shape, X_test.shape])\n",
    "\n",
    "print('Shape of label:')\n",
    "display([Y_train.shape, Y_val.shape, Y_test.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for i in range(0, 20):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(X_train[i, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Layers\n",
    "----\n",
    "In the [Recap 1](#Recap-1.-General-Layers) and [Recap 2](#Recap-2.-Activation-Layers), we've talk about the Dense layer and activation layer. Now we're going to talk more different type of layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layer\n",
    "Read the slide **week4.pptx** for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling Layer\n",
    "The pooling layer progressively reduces the spatial size of the representation. In the meanwhile, the number of parameters and computation in the network are also reduced. Typically there are 2 types of pooling: **max pooling** and **average pooling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(160, 160, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=40, validation_data=(X_val, Y_val), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['acc'], label='Training accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the Convolutional layer and pooling layer, the following layer types also play important roles in enhancing the training stability, preventing the over-fitting, and increasing the speed of convergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout Layer\n",
    "The dropout layer randomly masked out particular neurons. During the training procedures, the neuron is only kept active when its probability is higher than $p$. Otherwise, set it to 0.\n",
    "\n",
    "The randomness properties of the dropout layer prevent the attached layers always fit into particular parts of the input data.\n",
    "\n",
    "Notice that:\n",
    "- DO NOT place the dropout layer as the last layer of the neural network architecture, since it is not make sense to randomly mask the prediction result of some neurons.\n",
    "- The dropout layer prevents over-fitting only when the model capacity is sufficiently higher than the diversity of the data distributions. If the model capacity is not sufficient, the training performances would be dropped.\n",
    "- The dropout layer usually requires more epochs to reach the convergence.\n",
    "\n",
    "Now let's train with dropout layer added:\n",
    "\n",
    "> **Your task:** Adding the dropout layer, try the performance  \n",
    "> Remember to adjust the number of epochs and drop rate\n",
    "> \n",
    "> Reference: https://www.tensorflow.org/api_docs/python/tf/nn/dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Modify the following sequential model, by adding an additional dropout layer\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(160, 160, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=60, validation_data=(X_val, Y_val), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['acc'], label='Training accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization Layer\n",
    "Batch normalization layer fetches a group of data from the output of previous attached layer, centraling the value by minus the mean value $\\mu$, then divided by the variance $\\sigma$:\n",
    "$$\n",
    "\\tilde{x}_i = \\frac{x_i - \\mu}{\\sigma}\n",
    "$$\n",
    "where $\\mu = \\frac{1}{M} \\sum_{i=1}^{M} x_i $, and $\\sigma=\\sqrt{\\frac{1}{M} \\sum_{i=1}^M \\left(x_i - \\mu\\right)^2}$.\n",
    "\n",
    "Batch normalization resolves the saturation gradient for some activation function like **hyperbolic tangent** (tanh) and **sigmoid**. In other words, the training times can be reduced and a deep network architecture can be trainable.\n",
    "\n",
    "Notice that the batch size should be large enough. Mean and variances of small batch cannot reflect the data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(160, 160, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=60, validation_data=(X_val, Y_val), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['acc'], label='Training accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, check **week4.pptx**. Also, check following materials for more details:\n",
    "- http://cs231n.github.io/neural-networks-2/\n",
    "- http://cs231n.github.io/convolutional-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3. Transfer Learning with Pre-trained Model\n",
    "----\n",
    "\n",
    "Based on the [tensorflow's official documents](https://www.tensorflow.org/tutorials/images/transfer_learning):\n",
    "> A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as it is, or use transfer learning to customize this model to a given task.\n",
    "> \n",
    "> The intuition behind transfer learning is that if a model trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch training a large model on a large dataset.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/applications\n",
    "\n",
    "Now let's look at the original MobileNetV2's performance, trained on 14 million images of ImageNet:\n",
    "\n",
    "> **Your task:** Construct the base model of mobileNet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model = # Finish your task here\n",
    "base_model.summary()\n",
    "tf.keras.utils.plot_model(base_model, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_predict = base_model.predict(X_test)\n",
    "\n",
    "plt.imshow(X_test[0, :])\n",
    "print('Predicted:', tf.keras.applications.mobilenet_v2.decode_predictions(Y_test_predict, top=5)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your task:**\n",
    "> 1. Remove the last activation layer, which originally handles the 1000 classes classification problem\n",
    "> 2. Replacing by adding addtional 2 additional fully-connected layer\n",
    ">     - The number of unit of the second fully-connected layer should be 2, since its a cats and dogs classification\n",
    "> 3. Make all layers coming from mobileNet NOT trainable\n",
    "> 4. Re-train, and observe the training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish your task here\n",
    "\n",
    "model_transfer_learning.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model_transfer_learning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_transfer_learning.fit(X_train, Y_train, epochs=20, validation_data=(X_val, Y_val), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['acc'], label='Training accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Once finished, please Submit Your Colab Notebook [Here](https://forms.gle/Hkk7WS2BDsmwGovE8)**\n",
    "\n",
    "----\n",
    "**NOTICE:** It takes time to train a convolutional neural network model. For your convenience:\n",
    "- Lab1 is required, and\n",
    "- Select 1 of Lab2 or Lab3, and finish your task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
