{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Overfitting, Underfitting and Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please access this notebook at: https://github.com/sangaer/PracticalMachineLearning2019\n",
    "### Once finished, please [Submit Your Colab Notebook Here](https://forms.gle/sXpcqjZG5ycv7uYe8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "## recap for Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Neural Network**\n",
    "* Component must be differemtiable for optimization.\n",
    "* Network weights should be initialize with non-symmetry values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Loss Functions:**\n",
    "#### For classification:\n",
    "##### Cross Entropy:\n",
    "### $$ cross\\_entropy = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{c}^{C}y_{c}*log (y_{c\\_predict}) $$\n",
    "#### For Regression:\n",
    "##### MSE(Mean Square Error):\n",
    "## $$ MSE = \\frac{1}{2m}\\sum_{i=1}^{m}(y_{predict} - y)^2 $$\n",
    "\n",
    "\n",
    "\n",
    "## **Activation Functions:**\n",
    "### sigmoid:\n",
    "## $$ sigmoid(x) = \\frac{1}{1+e^{-x}} $$ \n",
    "#### $$ sigmoid(x)_j\\in (0, 1) $$\n",
    "\n",
    "### identity / linear\n",
    "## $$ linear(x) = x $$\n",
    "#### $$ linear(x)_j \\in (-\\infty, \\infty) $$\n",
    "\n",
    "### ReLU\n",
    "## $$ \n",
    "ReLU(x) = \\begin{cases}\n",
    "        x,  & \\text{for x >= 0}\\\\\n",
    "        0, & \\text{for x < 0}\n",
    "        \\end{cases} \n",
    "        $$\n",
    "#### $$ ReLU(x)_j \\in (0, \\infty) $$\n",
    "\n",
    "\n",
    "### Softmax\n",
    "## $$ Softmax(x)_i = \\frac {e^{x_i}}{\\sum _j e^{x_j}} $$\n",
    "#### $$ Softmax(x)_j \\in (0, 1) $$\n",
    "#### $$ \\sum_{j}Softmax(x)_j = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting today's task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.sum(np.multiply(np.log(predictions), targets)) / predictions.shape[0]\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "FILE_TRAIN = 'sample_data/mnist_train_small.csv'\n",
    "FILE_TEST = 'sample_data/mnist_test.csv'\n",
    "\n",
    "df_train = pd.read_csv(FILE_TRAIN).sample(frac=1).reset_index(drop=True)\n",
    "df_test = pd.read_csv(FILE_TEST).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "x_normalizer = StandardScaler()\n",
    "y_encoder = OneHotEncoder(categories='auto')\n",
    "\n",
    "x_train = x_normalizer.fit_transform(df_train.iloc[:, 1:])\n",
    "y_train = df_train.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_train_oh = y_encoder.fit_transform(y_train).toarray()\n",
    "\n",
    "x_test = x_normalizer.transform(df_test.iloc[:, 1:])\n",
    "y_test = df_test.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_test_oh = y_encoder.transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "# Overfitting and Underfitting\n",
    "[wikipedia](https://en.wikipedia.org/wiki/Overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When fitting on training set:\n",
    "![](https://drive.google.com/uc?id=1yAuC-Ky6UqMehA5heZ3py0c4I_AZo1Iy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting\n",
    "#### How:\n",
    "* Underfitting occurs when a statistical model or machine learning algorithm cannot adequately capture the underlying structure of the data. \n",
    "* It occurs when the model or algorithm does not fit the data enough. \n",
    "* It is often a result of an excessively simple model.\n",
    "\n",
    "#### Solution:\n",
    "* Try a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(4, input_shape=(x_train.shape[1], ), activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss)\n",
    "\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "for i in range(120):\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              verbose=0, \n",
    "              epochs=1, \n",
    "              validation_data=(x_test, y_test), \n",
    "              batch_size=256,\n",
    "              shuffle=True)\n",
    "    \n",
    "    train_losses.append(cross_entropy(model.predict(x_train), y_train_oh))\n",
    "    val_losses.append(cross_entropy(model.predict(x_test), y_test_oh))\n",
    "    \n",
    "    \n",
    "    clear_output()    \n",
    "    plt.plot(train_losses, label='train loss')\n",
    "    plt.plot(val_losses, label='val loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "#### How:\n",
    "* The possibility of overfitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. \n",
    "* A model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data\n",
    "* Overfitting occurs when a model begins to \"memorize\" training data rather than \"learning\" to generalize from a trend.\n",
    "\n",
    "#### Solution:\n",
    "* Starts from simple.\n",
    "* Try a simpler model when overfitting.\n",
    "* Regularizaion (give constrain to limit the fitting ability for model).\n",
    "* Ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(2048, input_shape=(x_train.shape[1], ), activation='relu'),\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss)\n",
    "\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "for i in range(20):\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              verbose=0, \n",
    "              epochs=1, \n",
    "              validation_data=(x_test, y_test), \n",
    "              batch_size=256,\n",
    "              shuffle=True)\n",
    "    \n",
    "    train_losses.append(cross_entropy(model.predict(x_train), y_train_oh))\n",
    "    val_losses.append(cross_entropy(model.predict(x_test), y_test_oh))\n",
    "    \n",
    "    \n",
    "    clear_output()    \n",
    "    plt.plot(train_losses, label='train loss')\n",
    "    plt.plot(val_losses, label='val loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=(x_train.shape[1], ), activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss)\n",
    "\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "for i in range(120):\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              verbose=0, \n",
    "              epochs=1, \n",
    "              validation_data=(x_test, y_test), \n",
    "              batch_size=256,\n",
    "              shuffle=True)\n",
    "    \n",
    "    train_losses.append(cross_entropy(model.predict(x_train), y_train_oh))\n",
    "    val_losses.append(cross_entropy(model.predict(x_test), y_test_oh))\n",
    "    \n",
    "    \n",
    "    clear_output()    \n",
    "    plt.plot(train_losses, label='train loss')\n",
    "    plt.plot(val_losses, label='val loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "* Inference the final result by aggregate the inference from multiple models.\n",
    "\n",
    "### $$ prediction = aggregation (model1(x), model2(x), model3(x), ...) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout (for Neural Networks)\n",
    "[reference](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "\n",
    "![](https://drive.google.com/uc?id=19O_lTVcOjH1aQRqJRmaKzNoTKglG5rmz)\n",
    "#### For training:\n",
    "* Random drop neurons out by predefined rate. \n",
    "\n",
    "#### For prediction\n",
    "* Weight neuron values by dropout rate in training. \n",
    "\n",
    "\n",
    "#### NOTE:\n",
    "* Usually we will not apply dropout before first layer as it will drop input feature directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "DROP_RATE = 0.3\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=(x_train.shape[1], ), activation='relu'),\n",
    "    tf.keras.layers.Dropout(DROP_RATE),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(DROP_RATE),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss)\n",
    "\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "for i in range(120):\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              verbose=0, \n",
    "              epochs=1, \n",
    "              validation_data=(x_test, y_test), \n",
    "              batch_size=256,\n",
    "              shuffle=True)\n",
    "    \n",
    "    train_losses.append(cross_entropy(model.predict(x_train), y_train_oh))\n",
    "    val_losses.append(cross_entropy(model.predict(x_test), y_test_oh))\n",
    "    \n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    plt.plot(train_losses, label='train loss')\n",
    "    plt.plot(val_losses, label='val loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## L2 Regularization\n",
    "\n",
    "## $$ loss ← loss + \\lambda \\sum_{}|W|^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=(x_train.shape[1], ), \n",
    "                          activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "                         ),\n",
    "    tf.keras.layers.Dense(64, \n",
    "                          activation='relu',\n",
    "                         kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss)\n",
    "\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "for i in range(120):\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              verbose=0, \n",
    "              epochs=1, \n",
    "              validation_data=(x_test, y_test), \n",
    "              batch_size=256,\n",
    "              shuffle=True)\n",
    "    \n",
    "    train_losses.append(cross_entropy(model.predict(x_train), y_train_oh))\n",
    "    val_losses.append(cross_entropy(model.predict(x_test), y_test_oh))\n",
    "    \n",
    "    \n",
    "    clear_output()    \n",
    "    plt.plot(train_losses, label='train loss')\n",
    "    plt.plot(val_losses, label='val loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## L1 Regularization\n",
    "\n",
    "## $$ loss ← loss + \\lambda \\sum_{}|W| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=(x_train.shape[1], ), \n",
    "                          activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1(1e-5)\n",
    "                         ),\n",
    "    tf.keras.layers.Dense(64, \n",
    "                          activation='relu',\n",
    "                         kernel_regularizer=tf.keras.regularizers.l1(1e-5)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss)\n",
    "\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "for i in range(120):\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              verbose=0, \n",
    "              epochs=1, \n",
    "              validation_data=(x_test, y_test), \n",
    "              batch_size=256,\n",
    "              shuffle=True)\n",
    "    \n",
    "    train_losses.append(cross_entropy(model.predict(x_train), y_train_oh))\n",
    "    val_losses.append(cross_entropy(model.predict(x_test), y_test_oh))\n",
    "    \n",
    "    \n",
    "    clear_output()    \n",
    "    plt.plot(train_losses, label='train loss')\n",
    "    plt.plot(val_losses, label='val loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## Early Stopping\n",
    "\n",
    "* Stop training when validation result converages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "EARLY_STOP_IF_N_EPOCHS_NOT_IMPROVE = 3\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=(x_train.shape[1], ), activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss)\n",
    "\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "for i in range(1000):\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              verbose=0, \n",
    "              epochs=1, \n",
    "              validation_data=(x_test, y_test), \n",
    "              batch_size=256,\n",
    "              shuffle=True)\n",
    "    \n",
    "    train_losses.append(cross_entropy(model.predict(x_train), y_train_oh))\n",
    "    val_losses.append(cross_entropy(model.predict(x_test), y_test_oh))\n",
    "\n",
    "    clear_output()    \n",
    "    plt.plot(train_losses, label='train loss')\n",
    "    plt.plot(val_losses, label='val loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    if (len(val_losses) > EARLY_STOP_IF_N_EPOCHS_NOT_IMPROVE) and (np.argmin(val_losses[-(EARLY_STOP_IF_N_EPOCHS_NOT_IMPROVE+1):])==0):\n",
    "        print('Early stop at {}th iteration as val loss was not improved for {} epoch'.format(\n",
    "            i,\n",
    "            EARLY_STOP_IF_N_EPOCHS_NOT_IMPROVE\n",
    "        ))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply early stop as [keras callback function](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "EARLY_STOP_IF_N_EPOCHS_NOT_IMPROVE = 3\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=(x_train.shape[1], ), activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=EARLY_STOP_IF_N_EPOCHS_NOT_IMPROVE+1, min_delta=1e-20)\n",
    "model.fit(x_train, y_train, epochs=1000, callbacks=[callback], validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "### Score(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "\n",
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓ You can modify this block for better learning curve. ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "\n",
    "EARLY_STOP_IF_N_EPOCHS_NOT_IMPROVE = 1000\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(x_train.shape[1], ), activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss)\n",
    "\n",
    "for i in range(1000):\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              verbose=0, \n",
    "              epochs=1, \n",
    "              validation_data=(x_test, y_test), \n",
    "              batch_size=256,\n",
    "              shuffle=True)\n",
    "\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑ You can modify this block for better learning curve. ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "\n",
    "    train_losses.append(cross_entropy(model.predict(x_train), y_train_oh))\n",
    "    val_losses.append(cross_entropy(model.predict(x_test), y_test_oh))\n",
    "\n",
    "    clear_output()    \n",
    "    plt.plot(train_losses, label='train loss')\n",
    "    plt.plot(val_losses, label='val loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    if (len(val_losses) > EARLY_STOP_IF_N_EPOCHS_NOT_IMPROVE) and (np.argmin(val_losses[-(EARLY_STOP_IF_N_EPOCHS_NOT_IMPROVE+1):])==0):\n",
    "        print('Early stop at {}th iteration as val loss was not improved for {} epoch'.format(\n",
    "            i,\n",
    "            EARLY_STOP_IF_N_EPOCHS_NOT_IMPROVE\n",
    "        ))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "# Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "from copy import copy\n",
    "from itertools import product\n",
    "from  warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## Grid Search\n",
    "* search for the predefined meta-parameter from predefined searching space(s)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14\n",
    "x_valid = copy(x_test)\n",
    "y_valid = copy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15\n",
    "best_validation = 0\n",
    "best_model = None\n",
    "\n",
    "\n",
    "penalty_space = ['l1', 'l2']\n",
    "lr_space = [0.01, 0.03, 0.1, 0.3, 1]\n",
    "\n",
    "for penalty, lr in product(penalty_space, lr_space):\n",
    "    model = SGDClassifier(eta0=lr, max_iter=10, penalty=penalty)\n",
    "    model.fit(x_train, y_train.reshape(-1))\n",
    "    accuracy = accuracy_score(model.predict(x_valid), y_valid.reshape(-1))\n",
    "    \n",
    "    print('val accuracy: {} with penalty:{}, lr: {}'.format(accuracy, penalty, lr))\n",
    "    if best_validation < accuracy:\n",
    "        best_model = model\n",
    "        best_validation = accuracy\n",
    "\n",
    "print('\\n----\\nSearched best val accuracy: {:.4f} as model : {}'.format(best_validation, best_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## Cross Validation\n",
    "[sklearn document](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16\n",
    "lr = best_model.eta0\n",
    "model = SGDClassifier(eta0=lr, max_iter=10, penalty=penalty)\n",
    "val_accuracies = cross_val_score(model, x_train, y_train.reshape(-1), cv=5)\n",
    "mean_val_accuracy = np.mean(val_accuracies)\n",
    "print('mean_val_accuracy: {:.4f}'.format(mean_val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## [Grid Search with Cross Validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Grid Search CV classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17\n",
    "penalty_space = ['l1', 'l2']\n",
    "lr_space = [0.01, 0.03, 0.1, 0.3, 1]\n",
    "model = SGDClassifier(max_iter=10)\n",
    "\n",
    "param_grid = {'eta0': lr_space, 'penalty': penalty_space}\n",
    "clf = GridSearchCV(model, param_grid, n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit as normal clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18\n",
    "clf.fit(x_train, y_train.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate as normal clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19\n",
    "y_predict = clf.predict(x_test)\n",
    "accuracy_score(y_predict, y_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "### Score(2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20\n",
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓ You can modify this block for better learning curve. ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "model = None\n",
    "param_grid = {}\n",
    "clf = GridSearchCV(model, param_grid, n_jobs=-1, cv=5)\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑ You can modify this block for better learning curve. ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "\n",
    "y_predict = clf.predict(x_test)\n",
    "accuracy_score(y_predict, y_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    "#### For more search algorithmns, please search for:\n",
    "* Random Hyperparameter Search\n",
    "* Bayesian Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "### Once finished, please [Submit Your Colab Notebook Here](https://forms.gle/sXpcqjZG5ycv7uYe8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For next week(17, Dec, 2019), Jessee will introduce more Neural Network architecture for solving more general data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date +%Y%m%d_%H%M%S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
