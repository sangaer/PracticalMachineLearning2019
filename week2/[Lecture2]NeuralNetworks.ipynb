{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 : Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please access this notebook at: https://github.com/sangaer/PracticalMachineLearning2019\n",
    "### Once finished, please [Submit Your Colab Notebook Here](https://forms.gle/RjqyyLsVH9Qgf4VaA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recap for Lecture 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle\n",
    "access kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d tongpython/cat-and-dog\n",
    "!unzip -q cat-and-dog.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv dataset\n",
    "FILE_TRAIN = 'sample_data/california_housing_train.csv'\n",
    "FILE_TEST = 'sample_data/california_housing_test.csv'\n",
    "df_train = pd.read_csv(FILE_TRAIN)\n",
    "df_test = pd.read_csv(FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glace\n",
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column names\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a column as \"Series\"\n",
    "longitude = df_train['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing\n",
    "df_train.iloc[10:15, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array declare\n",
    "\n",
    "# toy data\n",
    "x_train = np.array([[1, 40], \n",
    "                    [2, 30],\n",
    "                    [4, 10]])\n",
    "\n",
    "y_train = np.array([2, 3, 5])\n",
    "\n",
    "x_test = np.array([[3, 20],])\n",
    "y_test = np.array([4])\n",
    "\n",
    "\n",
    "# toy model\n",
    "w = np.array([1, 0])\n",
    "b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrix transpose\n",
    "x_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner product, boardcasting\n",
    "np.dot(w, x_train.T) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elementwise multiply\n",
    "x_train * x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###               scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "normalizer = StandardScaler()\n",
    "x_train_norm = normalizer.fit_transform(x_train)\n",
    "x_train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(normalizer.fit_transform(x_train), y_train)\n",
    "predict = reg.predict(normalizer.transform(x_test))\n",
    "\n",
    "print('\\nx_train: \\n{}\\n\\ny_train: \\n{}'.format(x_train, y_train))\n",
    "print('\\nx_test: \\n{}\\n\\npredict: \\n{}'.format(x_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "## Train / Valid / Test split\n",
    "Pattern Recognition and Neural Networks (Brian D. Ripley, 1996)\n",
    "#### **Training Set:**\n",
    "       - A set of examples used for learning, which is to fit the parameters [i.e., weights] of the classifier.\n",
    "        \n",
    "        \n",
    "#### - **Validation Set:** \n",
    "       - A set of examples used to tune the parameters [i.e., architecture, not weights] of a classifier, for example to choose the number of hidden units in a neural network.\n",
    "        \n",
    "        \n",
    "#### - **Test Set:** \n",
    "       - A set of examples used only to assess the performance [generalization] of a fully specified classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict: \n",
    "## $$ Z = f(W^TX + b) $$ \n",
    "\n",
    "#### optimize:\n",
    "## $$ W \\leftarrow W - learning\\_rate * \\frac{\\partial loss}{\\partial W} $$\n",
    "## $$ b \\leftarrow b - learning\\_rate * \\frac{\\partial loss}{\\partial b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  \n",
    "# Neural Networks from craft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward propagation\n",
    "#### NOTE: We need activation functions for breaking the linearity\n",
    "\n",
    "![ref: wikipidia MLP](https://drive.google.com/uc?id=1-3sYn5zZMHejRQHg33ngdRNOGGvsmGmt)\n",
    "### $$ Z_{0} = f_{0}(W_{0}^TX_{0} + b_{0}) $$ \n",
    "### $$ Z_{1} = f_{1}(W_{1}^TZ_{0} + b_{1}) $$ \n",
    "### $$ Z_{2} = f_{2}(W_{2}^TZ_{1} + b_{2}) $$ \n",
    "$$.$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "### $$ Z_{n} = f_{n}(W_{n}^TZ_{n-1} + b_{n}) $$ \n",
    "#### \\begin{cases}\n",
    "        b_n: bias\\\\\n",
    "        W_n: weight\\\\\n",
    "        X_n: layer\\_input\\\\\n",
    "        f_n: activation\\_functions\\\\\n",
    "        Z_n: layer\\_output\n",
    "        \\end{cases} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backpropagation\n",
    "#### NOTE\n",
    "* \"Derivative\" is necessary for optimize with backpropagation.\n",
    "* \"Weights\" should be initialize with non-symmetry(we usually initialize them in random) parameters is necessary for optimize with backpropagation.\n",
    "#### [appendix: Backpropagation](https://en.wikipedia.org/wiki/Backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    \n",
    "    weights = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, layer_x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abstractmethod\n",
    "    def derivative(self, layer_x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def w(self, layer_x):\n",
    "        return self.weights if self.weights is not None else np.eye(layer_x.shape[1])\n",
    "    \n",
    "    def backward(self, layer_x, grad_output):\n",
    "        return np.dot(grad_output, self.w(layer_x).T) * self.derivative(layer_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypeError is expected here. You cannot instantiate class with abstract methods.\n",
    "layer = Layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation functions\n",
    "reference: [wikipedia](https://en.wikipedia.org/wiki/Activation_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "## $$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "## $$ \\sigma'(x) = \\sigma(x)(1-\\sigma(x))$$\n",
    "#### \\begin{cases}\n",
    "        \\sigma(\\infty) = 1\\\\\n",
    "        \\sigma(0) = 0.5\\\\\n",
    "        \\sigma(-\\infty) = 0\n",
    "        \\end{cases} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def forward(self, layer_x):\n",
    "        return 1/(1+np.exp(-layer_x))\n",
    "    \n",
    "    def derivative(self, layer_x):\n",
    "        return self.forward(layer_x) * (1-self.forward(layer_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "y =  Sigmoid().forward(x)\n",
    "y_ = Sigmoid().derivative(x)\n",
    "plt.plot(x,y, label='sigmoid(x)')\n",
    "plt.plot(x,y_, label='sigmoid\\'(x)')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "#  \n",
    "\n",
    "## Identity\n",
    "## $$ I(x) = x $$\n",
    "## $$ I'(x) = 1 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def forward(self, layer_x):\n",
    "        return layer_x\n",
    "    \n",
    "    def derivative(self, layer_x):\n",
    "        return np.ones(layer_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000).reshape(-1, 1)\n",
    "y = Identity().forward(x)\n",
    "y_ = Identity().derivative(x)\n",
    "plt.plot(x, y, label='Identity(x)')\n",
    "plt.plot(x, y_, label='Identity\\'(x)')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "#  \n",
    "\n",
    "## Rectified Linear Unit (ReLU)\n",
    "## $$ \n",
    "ReLU(x) = \\begin{cases}\n",
    "        x,  & \\text{for x >= 0}\\\\\n",
    "        0, & \\text{for x < 0}\n",
    "        \\end{cases} \n",
    "        $$\n",
    "## $$ \n",
    "ReLU'(x) = \\begin{cases}\n",
    "        1,  & \\text{for x > 0}\\\\\n",
    "        0, & \\text{for x < 0}\n",
    "        \\end{cases} \n",
    "        $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \n",
    "    def forward(self, layer_x):\n",
    "        layer_y = np.maximum(0, layer_x)\n",
    "        return layer_y\n",
    "\n",
    "    def derivative(self, layer_x):\n",
    "        return layer_x > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000).reshape(-1, 1)\n",
    "y = ReLU().forward(x)\n",
    "y_ = ReLU().derivative(x)\n",
    "plt.plot(x, y, label='ReLU(x)')\n",
    "plt.plot(x, y_, label='ReLU\\'(x)')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Softmax and logit\n",
    "#### logit: not normalized probability distribution.\n",
    "\n",
    "## $$ Softmax(x)_i = \\frac {e^{x_i}}{\\sum _j e^{x_j}} $$\n",
    "#### $$softmax(x+c)_i = \\frac {e^{(x_i+c)}}{\\sum _j e^{(x_j+c)}} = \\frac {e^{c}e^{x_i}}{e^{c}\\sum _j e^{x_j}} = \\frac {e^{x_i}}{\\sum _j e^{x_j}} = softmax(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    \n",
    "    def forward(self, layer_x):\n",
    "        return np.exp(layer_x) / np.exp(layer_x).sum(axis=-1, keepdims=True)    \n",
    "\n",
    "    def derivative(self, layer_x):\n",
    "        return layer_x * (1 - layer_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = np.array([[1.0, 4.0], \n",
    "                  [2.0, 2.0],\n",
    "                  [2.1, 1.9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = logit.argmax(axis=-1)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propability = Softmax().forward(logit)\n",
    "propability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propability.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "# **Dense** (Feed-Forward Layer)\n",
    "#### a layer of gradient decent linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.normal(loc=0.0, \n",
    "                                        scale = np.sqrt(2/(input_units+output_units)), \n",
    "                                        size = (input_units,output_units))  # [xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self, layer_x):        \n",
    "        return np.dot(layer_x, self.weights) + self.biases\n",
    "    \n",
    "    def derivative(self, layer_x):\n",
    "        return 1\n",
    "    \n",
    "    def backward(self, layer_x, grad_output):\n",
    "        grad_input = np.dot(grad_output, self.w(layer_x).T) * self.derivative(layer_x)\n",
    "\n",
    "        grad_weights = np.dot(layer_x.T, grad_output)\n",
    "        grad_biases = grad_output.mean(axis=0) * layer_x.shape[0]\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape\n",
    "        assert grad_biases.shape == self.biases.shape\n",
    "\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for binary classification:\n",
    "$$ cross\\_entropy = -\\frac{1}{m}\\sum_{i=1}^{m} y*log(y_{predict}) + (1-y)*log(1-y_{predict})$$\n",
    "### for general case:\n",
    "$$ cross\\_entropy = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{c}^{C}y_{c}*log (y_{c\\_predict}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossentropyWithLogits:\n",
    "    \n",
    "    def softmax(self, layer_x):\n",
    "        return np.exp(layer_x) / np.exp(layer_x).sum(axis=-1, keepdims=True)    \n",
    "    \n",
    "    def loss(self, logits, reference_answers):\n",
    "        logits_for_answers = logits[np.arange(len(logits)), reference_answers]\n",
    "        xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "        return xentropy\n",
    "    \n",
    "    def gradient(self, logits, reference_answers):\n",
    "        ones_for_answers = np.zeros_like(logits)\n",
    "        ones_for_answers[np.arange(len(logits)), reference_answers] = 1\n",
    "        softmax = self.softmax(logits)\n",
    "        return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent / Mini-Batch Gradient Descent / Stochastic Gradient Descent(SGD)\n",
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "```python\n",
    "model.fit(x, y)\n",
    "```\n",
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "batch_size = 100\n",
    "x = np.zeros([500, 1])\n",
    "for b, i in enumerate(range(0, len(x), batch_size)):\n",
    "    print('batch {}  indices {}:{}'.format(b, i, i+batch_size))\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for i in range(0, len(x), batch_size):\n",
    "    model.fit(x[i: i+batch_size], y[i: i+batch_size])\n",
    "```\n",
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent(SGD)\n",
    "\n",
    "```python\n",
    "for i in range(0, len(x), 1):\n",
    "    model.fit(x[i: i+batch_size], y[i: i+batch_size])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "    \n",
    "    def __init__(self, layers, loss_function):\n",
    "        self.loss_function = loss_function\n",
    "        self.network = layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        activations = []\n",
    "        layer_x = x\n",
    "        for l in self.network:\n",
    "            activations.append(l.forward(layer_x))\n",
    "            layer_x = activations[-1]\n",
    "        assert len(activations) == len(self.network)\n",
    "        return activations\n",
    "\n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)[-1]\n",
    "        return logits.argmax(axis=-1)\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "\n",
    "        layer_activations = self.forward(x)\n",
    "        layer_inputs = [x] + layer_activations\n",
    "        logits = layer_activations[-1]\n",
    "\n",
    "        loss = self.loss_function.loss(logits, y)\n",
    "        return np.mean(loss)\n",
    "        \n",
    "    def train(self, x, y):\n",
    "\n",
    "        layer_activations = self.forward(x)\n",
    "        layer_inputs = [x] + layer_activations\n",
    "        logits = layer_activations[-1]\n",
    "\n",
    "        loss = self.loss_function.loss(logits, y)\n",
    "        loss_grad = self.loss_function.gradient(logits, y)\n",
    "\n",
    "        for layer_index in range(len(self.network))[::-1]:\n",
    "            layer = network[layer_index]\n",
    "\n",
    "            loss_grad = layer.backward(layer_inputs[layer_index],loss_grad)\n",
    "\n",
    "        return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat vs Dog dataset\n",
    "#### kaggle : tongpython/cat-and-dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_parse(folders, size=(32, 32)):\n",
    "    \n",
    "    if isinstance(folders, str):\n",
    "        folders = [folders, ]\n",
    "\n",
    "    features = list()\n",
    "    labels = list()\n",
    "\n",
    "    for folder in folders:\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            for f in filter(lambda x: x.endswith('.jpg'), files):\n",
    "                img = cv2.resize(cv2.imread(os.path.join(root, f)), size)\n",
    "                label = os.path.basename(root)\n",
    "                \n",
    "                labels.append(label)\n",
    "                features.append(img)\n",
    "\n",
    "    df = pd.DataFrame({'img':features, 'category': labels})\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "data_test ='test_set/test_set/'\n",
    "data_train ='training_set/training_set/'\n",
    "\n",
    "df = dataset_parse([data_test, data_train], size=(32, 32))\n",
    "\n",
    "print('df.shape:', df.shape)\n",
    "\n",
    "x = np.concatenate(df['img'].apply(lambda x: x.reshape(1, -1)).values) /256\n",
    "y = df['category'].apply(lambda x: 1 if x=='cats' else 0).values\n",
    "\n",
    "i = 9000\n",
    "j = 9200\n",
    "x_train = x[:i]\n",
    "y_train = y[:i]\n",
    "x_val = x[i:j]\n",
    "y_val = y[i:j]\n",
    "x_test = x[j:]\n",
    "y_test = y[j:]\n",
    "\n",
    "print('x_train.shape', x_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('x_val.shape', x_val.shape)\n",
    "print('y_val.shape', y_val.shape)\n",
    "print('x_test.shape', x_test.shape)\n",
    "print('y_test.shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓ You can modify this block for improve the performance. ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "network = list()\n",
    "network.append(Dense(x_train.shape[1], 32, learning_rate=learning_rate))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(32, 32, learning_rate=learning_rate))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(32, len(set(y_train)), learning_rate=learning_rate))\n",
    "\n",
    "clf = MLPClassifier(network, loss_function=SoftmaxCrossentropyWithLogits())\n",
    "\n",
    "\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑ You can modify this block for improve the performance. ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓ You can modify this block for improve the performance. ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "\n",
    "N_Epoch = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑ You can modify this block for improve the performance. ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs = list()\n",
    "val_accs = list()\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "\n",
    "for epoch in range(N_Epoch):\n",
    "    for x_batch, y_batch in iterate_minibatches(x_train, y_train, batchsize=BATCH_SIZE,shuffle=True):\n",
    "        train_loss = clf.train(x_batch, y_batch)\n",
    "    val_loss = clf.loss(x_val, y_val)\n",
    "    \n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    train_accs.append(np.mean(clf.predict(x_train)==y_train))\n",
    "    val_accs.append(np.mean(clf.predict(x_val)==y_val))\n",
    "    \n",
    "    \n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\", epoch)\n",
    "    print(\"Train loss:\", train_losses[-1])\n",
    "    print(\"Val loss:\", val_losses[-1])\n",
    "    print(\"Train accuracy:\", train_accs[-1])\n",
    "    print(\"Val accuracy:\", val_accs[-1])\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[0].plot(train_losses, label='train loss')\n",
    "    ax[0].plot(val_losses, label='val loss')\n",
    "    ax[0].legend(loc='best')\n",
    "    ax[0].grid()\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[1].plot(train_accs,label='train accuracy')\n",
    "    ax[1].plot(val_accs,label='val accuracy')\n",
    "    ax[1].legend(loc='best')\n",
    "    ax[1].set_ylabel('accuracy')\n",
    "    ax[1].grid()\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is ***overfitting***, when there are obvious leading performance from training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** SCORE 1/4 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(clf.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "# mnist dataset: \n",
    "#### handwriting numbers(0-9) classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('sample_data/mnist_train_small.csv').sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "df_test = pd.read_csv('sample_data/mnist_test.csv')\n",
    "\n",
    "x = df_train.iloc[:, 1:].values/256\n",
    "y = df_train.iloc[:, 0].values\n",
    "\n",
    "val_ratio = 0.1\n",
    "val_splitter = int(x.shape[0] * (1-val_ratio))\n",
    "\n",
    "x_train = x[:val_splitter]\n",
    "y_train = y[:val_splitter]\n",
    "x_val = x[val_splitter:]\n",
    "y_val = y[val_splitter:]\n",
    "\n",
    "x_test = df_test.iloc[:, 1:].values/256\n",
    "y_test = df_test.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "label = df_train.iloc[i, 0]\n",
    "plt.imshow(df_train.iloc[i, 1:].values.reshape(28, 28))\n",
    "plt.show()\n",
    "print('label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓ You should finish this block for improve the performance. ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "\n",
    "\n",
    "\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑ You should finish this block for improve the performance. ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓ You should finish this block for improve the performance. ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "N_Epoch = 0\n",
    "BATCH_SIZE = 0\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑ You should finish this block for improve the performance. ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs = list()\n",
    "val_accs = list()\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "\n",
    "for epoch in range(N_Epoch):\n",
    "    for x_batch, y_batch in iterate_minibatches(x_train, y_train, batchsize=BATCH_SIZE,shuffle=True):\n",
    "        train_loss = clf.train(x_batch, y_batch)\n",
    "    val_loss = clf.loss(x_val, y_val)\n",
    "    \n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    train_accs.append(np.mean(clf.predict(x_train)==y_train))\n",
    "    val_accs.append(np.mean(clf.predict(x_val)==y_val))\n",
    "    \n",
    "    \n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\", epoch)\n",
    "    print(\"Train loss:\", train_losses[-1])\n",
    "    print(\"Val loss:\", val_losses[-1])\n",
    "    print(\"Train accuracy:\", train_accs[-1])\n",
    "    print(\"Val accuracy:\", val_accs[-1])\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[0].plot(train_losses, label='train loss')\n",
    "    ax[0].plot(val_losses, label='val loss')\n",
    "    ax[0].legend(loc='best')\n",
    "    ax[0].grid()\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[1].plot(train_accs,label='train accuracy')\n",
    "    ax[1].plot(val_accs,label='val accuracy')\n",
    "    ax[1].legend(loc='best')\n",
    "    ax[1].set_ylabel('accuracy')\n",
    "    ax[1].grid()\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** SCORE 2/4 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(clf.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras within tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print('tf.__version__ = ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓ You can modify this block for improve the performance. ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "\n",
    "N_Epoch = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, input_shape=(x_train.shape[1], ), activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(set(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,             \n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=N_Epoch, verbose=2, validation_data=(x_val, y_val), batch_size=BATCH_SIZE)\n",
    "\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑ You can modify this block for improve the performance. ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Try some more optimizers! Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "### [Try some more loss functions!](https://www.tensorflow.org/api_docs/python/tf/keras/losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** SCORE 3/4 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(model.predict_classes(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing Price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = StandardScaler()\n",
    "y_normalizer = StandardScaler()\n",
    "\n",
    "df_train = pd.read_csv('sample_data/california_housing_train.csv').sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "df_test = pd.read_csv('sample_data/california_housing_test.csv')\n",
    "\n",
    "x = df_train.iloc[:, :-1].values\n",
    "y = df_train.iloc[:, -1].values\n",
    "\n",
    "val_ratio = 0.1\n",
    "val_splitter = int(x.shape[0] * (1-val_ratio))\n",
    "\n",
    "x_train = normalizer.fit_transform(x[:val_splitter])\n",
    "y_train = y_normalizer.fit_transform(y[:val_splitter].reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "x_val = normalizer.transform(x[val_splitter:])\n",
    "y_val = y_normalizer.transform(y[val_splitter:].reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "x_test = normalizer.transform(df_test.iloc[:, :-1])\n",
    "y_test = y_normalizer.transform(df_test.iloc[:, -1].values.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "print('x_train.shape', x_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('x_val.shape', x_val.shape)\n",
    "print('y_val.shape', y_val.shape)\n",
    "print('x_test.shape', x_test.shape)\n",
    "print('y_test.shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓ You should finish this block for improve the performance. ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "\n",
    "N_Epoch = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑ You should finish this block for improve the performance. ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    assert  len(list(filter(lambda x: x!=1, y_true.shape)))==1\n",
    "    assert  len(list(filter(lambda x: x!=1, y_pred.shape)))==1\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    assert  len(list(filter(lambda x: x!=1, y_true.shape)))==1\n",
    "    assert  len(list(filter(lambda x: x!=1, y_pred.shape)))==1\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean((y_true - y_pred) / y_true) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** SCORE 4/4 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = y_normalizer.inverse_transform(model.predict(x_test))\n",
    "y_test_ = y_normalizer.inverse_transform(y_test)\n",
    "\n",
    "for metrix in [mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, mean_percentage_error]:\n",
    "    print('{}: {}'.format(metrix.__name__, metrix(y_test_, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "#  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once finished, please [Submit Your Colab Notebook Here](https://forms.gle/RjqyyLsVH9Qgf4VaA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next lecture(10, Dec, 2019), we will introduce some trainning issues(e.g. underfitting, overfitting) and techniques for dealing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date +%Y%m%d_%H%M%S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
